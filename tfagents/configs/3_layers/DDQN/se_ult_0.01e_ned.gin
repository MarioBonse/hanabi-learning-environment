# Agent.
create_agent.agent_class = 'DDQN'
QNetwork.fc_layer_params = (512, 512, 512)
create_agent.learning_rate = 1e-7
create_agent.gradient_clipping = 0.1
create_agent.target_update_tau = 0.05
create_agent.target_update_period = 5
create_agent.gamma = 0.99
create_agent.reward_scale_factor = 1.0
create_agent.debug_summaries = False
create_agent.summarize_grads_and_vars = False


# Replay Buffer
create_replay_buffer.rb_type = 'uniform'
create_replay_buffer.max_length = 50000    # RB capacity


# Decaying Epsilon for the Epsilon-Greedy policy
train_eval.initial_epsilon = 0.01
train_eval.decay_type = None
train_eval.decay_time = 360
train_eval.reset_at_step = None         # can be None


# Running and Training process
train_eval.num_iterations = 72000
train_eval.collect_episodes_per_epoch = 10
train_eval.train_steps_per_epoch = 200
train_eval.num_eval_episodes = 1000
train_eval.batch_size = 64


# Checkpointing and Evaluation
train_eval.train_checkpoint_interval = 12000
train_eval.policy_checkpoint_interval = 12000
train_eval.rb_checkpoint_interval = 12000
train_eval.eval_interval = 3600


# Environment
create_environment.game_type = 'Hanabi-Full-CardKnowledge'
create_environment.num_players = 2

